{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ebc7bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#Visualization imports\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "\n",
    "#NLP imports\n",
    "import spacy\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer, TweetTokenizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import classification_report, roc_curve, roc_auc_score, plot_confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import recall_score, f1_score, accuracy_score, precision_score\n",
    "import heapq\n",
    "import re\n",
    "import nltk\n",
    "import networkx as nx\n",
    "from gensim.models import word2vec\n",
    "\n",
    "#Scikit Learn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc32202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading Data\n",
    "df = pd.read_csv('../data/train.csv')\n",
    "df_test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b283833",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(0, len(df)):\n",
    "    review = re.sub(r\"http\\S+\", \"\", df['text'][i]) # This code removes links from text\n",
    "    review = re.sub('[^a-zA-Z\\d+]', ' ', review) \n",
    "    review = re.sub('[0-9]', '', review) \n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [lemmatizer.lemmatize(word, pos = 'v') for word in review if not word in stopwords.words('english')]\n",
    "    review = [lemmatizer.lemmatize(word, pos = 'n') for word in review]\n",
    "    review = [lemmatizer.lemmatize(word, pos = 'a') for word in review] \n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c73bde8",
   "metadata": {},
   "source": [
    "### Baseline Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15f32df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].value_counts(normalize = True)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bad8c16",
   "metadata": {},
   "source": [
    "##### If the model has an accuracy score of more than 57% then the model is doing better than a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8184da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16348482",
   "metadata": {},
   "source": [
    "##### Looking at the first line of corpus we can see that the data is clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc19f74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a Dictionary to see most frequent words\n",
    "wordfreq = {}\n",
    "for sentence in corpus:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae95d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using heap module in python to see 10 most frequent words\n",
    "most_freq = heapq.nlargest(200, wordfreq, key=wordfreq.get)\n",
    "most_freq[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f818b147",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create features for Bag of words manually, counter vectorizer can do this for us\n",
    "sentence_vectors = []\n",
    "for sentence in corpus:\n",
    "    sentence_tokens = nltk.word_tokenize(sentence)\n",
    "    sent_vec = []\n",
    "    for token in most_freq:\n",
    "        if token in sentence_tokens:\n",
    "            sent_vec.append(1)\n",
    "        else:\n",
    "            sent_vec.append(0)\n",
    "    sentence_vectors.append(sent_vec)\n",
    "sentence_vectors = np.asarray(sentence_vectors)\n",
    "sentence_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4053515",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating Countervectorizer\n",
    "cv = CountVectorizer(max_features=1000, ngram_range = (1,2))\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1203c012",
   "metadata": {},
   "source": [
    "##### The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.\n",
    "[Ref](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb355e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Splitting data for training and test data and applying Naive Bayes Classification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 42)\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "y_pred_clf = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5570115",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set score using Naive Bayes Classifier: {:.2f}\".format(clf.score(X_train, y_train)))\n",
    "print(\"Testing set score using Naive Bayes Classifier: {:.2f}\" .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db16ca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix plot\n",
    "plot_confusion_matrix(clf,X_test, y_test, cmap = 'Blues')\n",
    "plt.title(\"Confusion Matrix using Naive Bayes Classifier\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01c1dde",
   "metadata": {},
   "source": [
    "##### This model predicts 329 False Negatives and 163 False Positive and the rest were predicted accurately and I used different parameter's like binary = true and ngram_range = (2,3) which resulted in less accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149019c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Predicted Probabilities\n",
    "y_pred_proba = clf.predict_proba(X_test)[:,1]\n",
    "y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121f876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy score using Naive Bayes Classifier: {round(accuracy_score(y_test, y_pred_clf),2)}')\n",
    "print(f'Recall score using Naive Bayes Classifier: {round(recall_score(y_test, y_pred_clf),2)}')\n",
    "print(f'F1 score using Naive Bayes Classifier: {round(f1_score(y_test, y_pred_clf),2)}')\n",
    "print(f'Precision score using Naive Bayes Classifier: {round(precision_score(y_test, y_pred_clf),2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3c8e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr,tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr, color = 'orange', lw =2)\n",
    "plt.plot([0, 1], [0, 1], lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.title(\"ROC Curve of Real and Fake tweets using NB Classifier\", color = 'blue')\n",
    "plt.xlabel('False Possitive Rate(1-Specificity)')\n",
    "plt.ylabel('True Possitive Rate(Sensitivity)')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd994a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The area under ROC CURVE using Naive Bayes {:.2f}\".format(roc_auc_score(y_test, y_pred_proba)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb35ca13",
   "metadata": {},
   "source": [
    "##### We can see that the model is definitely performing better than the baseline model and the Area under the curve is 0.84. AUC ranges from 0 to 1. Higher AUC means better perfomance of model in differentiating possitive and negative classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f151210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating Logistic Regression Model\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "print('Training Accuracy score using Logistic Regression:   {:.2f}'.format(train.score(X_train, y_train)))\n",
    "print('Test Accuracy score:   {:.2f}'.format(train.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c5ee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(lr, X_test, y_test, cmap = 'Blues')\n",
    "plt.title(\"Confusion Matrix using Logistic Regression\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a9ae40",
   "metadata": {},
   "source": [
    "##### It appears from the confusion matrix that the logistic regression model is classifying better than the Naive Bayes model, here the model predicts 195 False positives and 277 False Negatives. The results are similar to Naive Bayes classifier but slightly better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa31a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e421cc0",
   "metadata": {},
   "source": [
    "##### The F1score, accuracy, precision and recall scores are also slightly better than Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d26b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_lr = lr.predict_proba(X_test)[:,1]\n",
    "y_pred_proba_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bad473",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr,tpr, thresholds = roc_curve(y_test, y_pred_proba_lr)\n",
    "plt.plot(fpr,tpr, lw =2, color = 'orange')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.title(\"ROC Curve of real and fake tweets using Logistic Regression\")\n",
    "plt.xlabel('False Possitive Rate(1-Specificity)')\n",
    "plt.ylabel('True Possitive Rate(Sensitivity)')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c928a753",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The area under ROC CURVE using Logistic Regression {:.2f}\".format(roc_auc_score(y_test, y_pred_proba_lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2008a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating Random grid for RFC\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(i) for i in np.linspace(100, 1100, 100)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "# Max number of levels in tree\n",
    "max_depth = [None, 1,2,3,4,5,6,7]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [1,3,4,5,7,9]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [2,4,6,8]\n",
    "criterion = ['entropy', 'gini']\n",
    "rf_grid = {'n_estimators' : n_estimators,\n",
    "              'max_features': max_features,\n",
    "              'max_depth' : max_depth,\n",
    "              'min_samples_split' : min_samples_split,\n",
    "              'min_samples_leaf' : min_samples_leaf,\n",
    "              'criterion' : criterion}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f00184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf1 = RandomForestClassifier()\n",
    "rscv = RandomizedSearchCV(rf1, \n",
    "                          param_distributions = rf_grid, \n",
    "                          n_iter = 100,\n",
    "                          cv = 5,\n",
    "                          n_jobs = -1,\n",
    "                          verbose =2,\n",
    "                          random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda229f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rscv.fit(X_train, y_train)\n",
    "print(rscv.score(X_train, y_train))\n",
    "print(rscv.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1714109d",
   "metadata": {},
   "source": [
    "##### The Random Forest Clasifier with Randomseachcv took about 9 hours and the results were similar to previously performed Logistic Regression and Naive Bayes Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00476c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "rscv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e08b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate RFC with GridsearchCV\n",
    "rf = RandomForestClassifier()\n",
    "rf_params = {\n",
    "    'n_estimators': [400,500,600],\n",
    "    'max_depth': [None, 1, 2, 3, 4, 5, 6, 7],\n",
    "    'min_samples_leaf': [2],\n",
    "    'min_samples_split': [5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97445202",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gs = GridSearchCV(rf, param_grid=rf_params,\n",
    "                  n_jobs = -1,\n",
    "                  cv=5)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0cf497",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs.score(X_train, y_train))\n",
    "print(gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99003c0",
   "metadata": {},
   "source": [
    "##### using the best hyperparametres from randomsearchcv, gridsearchcv results in same accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaa6212",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_estimator_.feature_importances_[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff19703",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(X_train, \n",
    "                          columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cfef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d688ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'feature_names':X_train_df.columns,\n",
    "                   'feature_importance':gs.best_estimator_.feature_importances_})\n",
    "\n",
    "#Sort the DataFrame in order decreasing feature importance\n",
    "df1.sort_values(by=['feature_importance'], ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de326c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.set_index('feature_names', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6398f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1.sort_values(by = 'feature_importance', ascending = True).tail(10).plot(kind = 'barh', color = 'teal',\n",
    "                                                                            edgecolor = 'black',\n",
    "                                                                          figsize = (6,3))\n",
    "plt.title(\"Top Ten Features\")\n",
    "plt.ylabel('Feature Names')\n",
    "plt.savefig('./images/topfeatures.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9760a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('./data/topfeatures..csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31982b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d6028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['corpus'] = corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94178090",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['corpus']\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e31939",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ba57ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, \n",
    "                                                   random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd85823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating corpus for test data for predictions\n",
    "corpus_test = []\n",
    "for i in range(0, len(df_test)):\n",
    "    review = re.sub(r\"http\\S+\", \"\", df_test['text'][i])\n",
    "    review = re.sub('[^a-zA-Z\\d+]', ' ', review)\n",
    "    review = re.sub('[0-9]', '', review)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [lemmatizer.lemmatize(word, pos = 'v') for word in review if not word in stopwords.words('english')]\n",
    "    review = [lemmatizer.lemmatize(word, pos = 'n') for word in review]\n",
    "    review = [lemmatizer.lemmatize(word, pos = 'a') for word in review]\n",
    "    review = ' '.join(review)\n",
    "    corpus_test.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5057aef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline([\n",
    "    ('tf', TfidfVectorizer(max_features = 1000, ngram_range = (1,2), binary = True)),\n",
    "    ('lr_cv', LogisticRegression(C = 1.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984674c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d88ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train Accuracy score using TFIDF Logistic Regression: {round(pipe1.score(X_train, y_train),3)}')\n",
    "print(f'Test Accuracy score using TFIDF Logistic Regression: {round(pipe1.score(X_test, y_test),3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93eb958",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/logistic_tfidf.pkl', 'wb') as f:\n",
    "    pickle.dump(pipe1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9cd6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad5b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting on test set\n",
    "pipe1.predict(corpus_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6513aa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(pipe1, X_test, y_test, cmap = 'Blues')\n",
    "plt.title(\"Confusion Matrix using Logistic Regression with 1.5 Penalty\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66674ddb",
   "metadata": {},
   "source": [
    "##### The Logistic Regression model  using TFIDF vectorizer with 100 max features, ngram range of 1,2, l2 penalty with a regularization strength of 1.5 performed similar to our other models and predicted 172 False Positives, 286 False Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a10d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predlr = pipe1.predict(X_test)\n",
    "y_predlr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b25554",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking whether various thresholds can make a difference to our model\n",
    "my_threshold = 0.6\n",
    "y_preds_tr2 = (y_predlr > my_threshold)\n",
    "confusion_matrix(y_test, y_preds_tr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bfbd6d",
   "metadata": {},
   "source": [
    "##### There is no difference in prediction after changing the threshold to 0.6 and above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77337f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking whether various thresholds can make a difference to our model\n",
    "my_threshold = 0.6\n",
    "y_preds_tr2 = (y_predlr < my_threshold)\n",
    "confusion_matrix(y_test, y_preds_tr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649cca5d",
   "metadata": {},
   "source": [
    "##### Changing the threshold to 0.6 and below effected the model and it completely reversed the model predictions. It appears that changing the thresholds does not help our model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c5fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1.predict_proba(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4d5c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24335db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1.predict(corpus_test)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c15a983",
   "metadata": {},
   "source": [
    "##### It appears that our above model predicts \"happen terrible car crash\" as real Disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9fbd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3,\n",
    "                                                   random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b417addb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Pipeline with XGBoost Classifier\n",
    "pipe2 = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('xgb', XGBClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40f0580",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2_params = {\n",
    "    'cvec__max_features': [500, 1000, 2_000, 3_000],\n",
    "    'cvec__min_df': [0,1],\n",
    "    'cvec__max_df': [.9, .95],\n",
    "    'cvec__ngram_range': [(1, 1), (1, 2)],\n",
    "    'xgb__n_estimators': [100, 200, 300, 600],\n",
    "    'xgb__max_depth': [None, 1, 2, 3]\n",
    "    \n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV.\n",
    "\n",
    "gs1= GridSearchCV(pipe2, \n",
    "                  n_jobs = -1,\n",
    "                  param_grid = pipe2_params,\n",
    "                  cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6e03af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gs1.fit(X_train, y_train)\n",
    "print(f'Training Accuracy Score using XGBoost Classifier is : {round(gs1.score(X_train, y_train),2)}')\n",
    "print(\"----------------\")\n",
    "print(f'Testing Accuracy Score using XGBoost Classifier is : {round(gs1.score(X_test, y_test),2)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9a857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Estimator\n",
    "gs1.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c910532",
   "metadata": {},
   "source": [
    "##### max_df is the upper ceiling value of the frequency values, while min_df is just the lower cutoff value of the frequency values. If we want to remove more common words, we set max_df to a lower ceiling value between 0 and 1. If we want to remove more rare words, we set min_df to a higher cutoff value between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c8ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating Countervectorizer\n",
    "tf = TfidfVectorizer(max_df=0.9, max_features=1000, min_df = 1,\n",
    "                                 ngram_range=(1, 2))\n",
    "X = tf.fit_transform(corpus).toarray()\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc71d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, \n",
    "                                                    random_state = 42,\n",
    "                                                   stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df0eddb",
   "metadata": {},
   "source": [
    "##### After each boosting step, we can directly get the weights of new features, and 'eta' shrinks the feature weights to make the boosting process more conservative. The larger gamma is, the more conservative the algorithm will be.range: [0,âˆž]. Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b0c775",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(n_estimators = 2000, eta = 0.3, gamma = 5, max_depth = 8, subsample = 0.5)\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35248631",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(xgb.score(X_train, y_train),3))\n",
    "print(round(xgb.score(X_test, y_test),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e1a395",
   "metadata": {},
   "source": [
    "##### The XGBoost Classifier with TFIDF Vectorizer has a high accuracy score of 86% and a test accuracy of 78% and it is clear from the scores that the model is overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11134e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Dataframe to get feature names\n",
    "X_train_df1 = pd.DataFrame(X_train, \n",
    "                          columns= tf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d5378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'feature_names':X_train_df1.columns,\n",
    "                   'feature_importance':xgb.feature_importances_})\n",
    "\n",
    "#Sort the DataFrame in order decreasing feature importance\n",
    "df2.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c6e892",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting feature names as index to the dataframe\n",
    "df2.set_index('feature_names', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8198a242",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.sort_values(by = 'feature_importance', ascending = True).tail(10).plot(kind = 'barh', \n",
    "                                                                           edgecolor = 'black',\n",
    "                                                                          figsize = (7,5))\n",
    "plt.ylabel('Feature Names')\n",
    "plt.title(\"Top 10 Features using XGBoost Classifier\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713309c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/xgboost_cv', 'wb') as f:\n",
    "    pickle.dump(xgb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4229a337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix plot\n",
    "plot_confusion_matrix(xgb,X_test, y_test, cmap = 'Blues')\n",
    "plt.title(\"Confusion Matrix using XGBoost Classifier\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fddb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.fit_transform(corpus).toarray()\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, \n",
    "                                                    random_state = 42,\n",
    "                                                   stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3806feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression with l1 penalty\n",
    "lrcv = LogisticRegression(solver = 'newton-cg', penalty = 'l2', C = 1)\n",
    "lrcv.fit(X_train, y_train)\n",
    "print(f'Training Score: {round(lrcv.score(X_train, y_train),2)}')\n",
    "print(f'Testing SCore: {round(lrcv.score(X_test, y_test),2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3f59f4",
   "metadata": {},
   "source": [
    "##### The logistic regression model with newton-cg solver, l2 penalty with a regularization of 1 with TFIDF vectorizer has accuracy similar to previous models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcec51b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating a confusion matrix plot using the logisticregression with l1 penalty\n",
    "plot_confusion_matrix(lrcv, X_test, y_test, cmap = 'Blues')\n",
    "plt.title(\"Confusion Matrix using Logistic Regression with newton-cg\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22518c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression with l1 penalty\n",
    "lrcv1 = LogisticRegression(solver = 'liblinear', penalty = 'l1', C = 1.5)\n",
    "lrcv1.fit(X_train, y_train)\n",
    "print(f'Accuracy Train score using Logistic Regression with L1 penalty is :{round(lrcv1.score(X_train, y_train),2)}')\n",
    "print('---------------')\n",
    "print(f'Accuracy Test score using Logistic Regression with L1 penalty is :{round(lrcv1.score(X_test, y_test),2)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acc7c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating a confusion matrix plot using the logisticregression with l1 penalty\n",
    "plot_confusion_matrix(lrcv1, X_test, y_test, cmap = 'Blues')\n",
    "plt.title(\"Confusion Matrix using Logistic Regression with L1 penalty\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7249750",
   "metadata": {},
   "source": [
    "##### We can see that there are 109 False positives and 334 False negatives in our predicted model, we can try to balance these errors by increasing or decreasing the thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1996d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypreds = lrcv1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b92fc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "fpr,tpr, thresholds = roc_curve(y_test, ypreds)\n",
    "plt.plot(fpr,tpr, color = 'orange', lw = 2)\n",
    "plt.plot([0, 1], [0, 1], lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.title(\"ROC Curve of Real and Fake tweets using Logistic Regression l1 penalty\")\n",
    "plt.xlabel('False Possitive Rate(1-Specificity)')\n",
    "plt.ylabel('True Possitive Rate(Sensitivity)')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4eaf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, ypreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4357b590",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_threshold = 0.6\n",
    "y_preds_tr = (lrcv1.predict(X_test) > my_threshold).astype(int)\n",
    "confusion_matrix(y_test, y_preds_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2812856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e520800f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
